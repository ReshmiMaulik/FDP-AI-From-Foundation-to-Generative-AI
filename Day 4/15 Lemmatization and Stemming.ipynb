{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7def06b4",
   "metadata": {},
   "source": [
    "Lemmatization and stemming are techniques used in natural language processing to reduce words to their base or root form. These techniques are particularly useful for text processing and analysis. Let's demonstrate how to perform lemmatization and stemming using the NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30888371",
   "metadata": {},
   "source": [
    "# Step 1: Install NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f90c591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\shrim\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shrim\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\shrim\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\shrim\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shrim\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrim\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a2b41",
   "metadata": {},
   "source": [
    "# Step 2: Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c640b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shrim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shrim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Lemmatization is a technique that reduces words to their base form.\n",
      "\n",
      "Lemmas:\n",
      "['Lemmatization', 'is', 'a', 'technique', 'that', 'reduces', 'word', 'to', 'their', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text for lemmatization\n",
    "text = \"Lemmatization is a technique that reduces words to their base form.\"\n",
    "\n",
    "# Tokenize the text using NLTK\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Print original text and lemmas\n",
    "print(\"Original Text:\", text)\n",
    "print(\"\\nLemmas:\")\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb85cf",
   "metadata": {},
   "source": [
    "# Step 3: Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfaf5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Stemming is a technique that reduces words to their root form.\n",
      "\n",
      "Stems:\n",
      "['stem', 'is', 'a', 'techniqu', 'that', 'reduc', 'word', 'to', 'their', 'root', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text for stemming\n",
    "text = \"Stemming is a technique that reduces words to their root form.\"\n",
    "\n",
    "# Tokenize the text using NLTK\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Perform stemming\n",
    "stems = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print original text and stems\n",
    "print(\"Original Text:\", text)\n",
    "print(\"\\nStems:\")\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dffefcb",
   "metadata": {},
   "source": [
    "#The Porter, Snowball, and Lancaster stemmers are three different algorithms for stemming, each with its own characteristics. Here's a brief overview of the differences between them:\n",
    "\n",
    "1. Porter Stemmer:\n",
    "Developer: Developed by Martin Porter.\n",
    "Characteristics:\n",
    "It's one of the oldest and most well-known stemming algorithms.\n",
    "It uses a set of heuristic rules to strip common suffixes from English words.\n",
    "It is relatively simple and efficient.\n",
    "Example:\n",
    "\"running\" → \"run\"\n",
    "\"happiness\" → \"happi\"\n",
    "\n",
    "\n",
    "2. Snowball Stemmer (Porter2 or English Stemmer):\n",
    "Developer: Also developed by Martin Porter but as part of the Snowball stemmer project.\n",
    "Characteristics:\n",
    "An extension of the Porter stemmer with some improvements.\n",
    "It supports multiple languages, not just English.\n",
    "It is more aggressive than the Porter stemmer.\n",
    "Example:\n",
    "\"running\" → \"run\"\n",
    "\"happiness\" → \"happi\"\n",
    "\n",
    "\n",
    "3. Lancaster Stemmer:\n",
    "Developer: Developed by Chris D. Paice.\n",
    "Characteristics:\n",
    "It is the most aggressive of the three stemmers.\n",
    "It uses a different set of rules compared to the Porter and Snowball stemmers.\n",
    "It tends to produce shorter stems, but they may not be valid words.\n",
    "Example:\n",
    "\"running\" → \"run\"\n",
    "\"happiness\" → \"happy\"\n",
    "Differences:\n",
    "Aggressiveness:\n",
    "\n",
    "Porter: Moderate\n",
    "Snowball: More aggressive than Porter\n",
    "Lancaster: Most aggressive\n",
    "Rules:\n",
    "\n",
    "Porter and Snowball share some similarities in rules but differ in specific details.\n",
    "Lancaster uses a different set of rules compared to Porter and Snowball.\n",
    "Efficiency:\n",
    "\n",
    "Porter and Snowball are more widely used due to their balance between aggressiveness and efficiency.\n",
    "Lancaster is the most aggressive but may be less efficient for certain tasks.\n",
    "Word Length:\n",
    "\n",
    "Lancaster tends to produce shorter stems but may result in stems that are not valid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5b4e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Stemming is a technique that reduces words to their base or root form.\n",
      "Porter Stemmed sentence: stem is a techniqu that reduc word to their base or root form .\n",
      "Snowball Stemmed sentence: stem is a techniqu that reduc word to their base or root form .\n",
      "Lancaster Stemmed sentence: stem is a techn that reduc word to their bas or root form .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create stemmers for Porter, Snowball, and Lancaster algorithms\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")  # SnowballStemmer supports multiple languages, \"english\" is specified here\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Stemming is a technique that reduces words to their base or root form.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Stem each word in the sentence using Porter, Snowball, and Lancaster stemmers\n",
    "porter_stemmed_sentence = \" \".join([porter_stemmer.stem(word) for word in words])\n",
    "snowball_stemmed_sentence = \" \".join([snowball_stemmer.stem(word) for word in words])\n",
    "lancaster_stemmed_sentence = \" \".join([lancaster_stemmer.stem(word) for word in words])\n",
    "\n",
    "# Print the results\n",
    "print(\"Original sentence:\", sentence)\n",
    "print(\"Porter Stemmed sentence:\", porter_stemmed_sentence)\n",
    "print(\"Snowball Stemmed sentence:\", snowball_stemmed_sentence)\n",
    "print(\"Lancaster Stemmed sentence:\", lancaster_stemmed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1ef39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
